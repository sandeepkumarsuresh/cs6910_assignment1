{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for Sigmoid Neuron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Sigmoid_Neuron:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def perceptron(self,x):\n",
    "        # here we are doing the dot product , Therefore check the \n",
    "        # shape of the array here\n",
    "        return np.dot(x,self.w.T) + self.b\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        # 1.0 is for floating for accuracy\n",
    "        return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "    def gradient_w(self,x,y):\n",
    "        y_pred = self.sigmoid(self.perceptron(x))\n",
    "        return (y_pred - y ) * y_pred * (1 - y_pred) * x\n",
    "\n",
    "    def gradient_b(self,x,y):\n",
    "        y_pred = self.sigmoid(self.perceptron(x))\n",
    "        return (y_pred - y ) * y_pred * (1 - y_pred) * x\n",
    "\n",
    "    def fit(self,X,Y,epochs=1 , lr =1):\n",
    "\n",
    "        # initializing w and b\n",
    "        self.w = np.random.randn(1,X.shape(1))\n",
    "        self.b = 0\n",
    "\n",
    "        for i in range(epochs):\n",
    "            dw = 0\n",
    "            dw = 0\n",
    "            for x , y in zip(X,Y):\n",
    "                dw += self.gradient_w(x,y)\n",
    "                db += self.gradient_b(x,y)\n",
    "            self.w -= lr * dw\n",
    "            self.b -= lr * db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification with hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN:\n",
    "    def __init__(self , inputs , hidden_states = [2]):\n",
    "        self.n_inputs = inputs\n",
    "        self.n_outputs = 1\n",
    "        self.n_hidden_layers = len(hidden_states)\n",
    "        self.sizes = {self.nx} + hidden_states + {self.n_outputs}\n",
    "\n",
    "        self.W = {}\n",
    "        self.B = {}\n",
    "\n",
    "        for i in range(self.n_hidden_layers + 1):\n",
    "            self.W[i+1] = np.random.randn[self.sizes[i] , self.sizes[i+1]]\n",
    "            self.B[i+1] = np.zeros((1,self.sizes[i+1]))\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        # 1.0 is for floating for accuracy\n",
    "        return 1.0/(1.0 + np.exp(-x))\n",
    "    \n",
    "    def forward_pass(self,x):\n",
    "        \"\"\"\n",
    "        In the forward pass , we are defining the code to predict the output\n",
    "\n",
    "        How things work :\n",
    "\n",
    "            1. We predict the output \n",
    "            2. We update the weights and bias\n",
    "        \"\"\"\n",
    "\n",
    "        self.A = {}\n",
    "        self.H = {}\n",
    "        self.H[0] = x.reshape(1,-1) # Here (1,-1) means that the we are reshaping x to have 1 row and as many columns\n",
    "        for i in range(self.n_hidden_layers +1):\n",
    "            self.A[i+1] = np.matmul(self.H[i],self.W[i+1]) + self.B[i+1]\n",
    "            self.H[i+1] = self.sigmoid(self.A[i+1])\n",
    "        return self.H[self.n_hidden_layers + 1]\n",
    "    \n",
    "    def grad_sigmoid(self,x):\n",
    "        return x*(1-x)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the basis for our assignment\n",
    "def forward_pass():\n",
    "    x = x.reshape(1,-1) \n",
    "    self.A1 = np.matmul(x,self.W1) + self.B1 # this is a regular matrix multiplications\n",
    "    self.H1 = self.sigmoid(self.A1)\n",
    "    self.a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding for Deep Learning Assignment1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here the \n",
    "    Input is 784 \n",
    "    Output is 10 classes\n",
    "\n",
    "All the other parameters are initialised by the user/hardcoded\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class NN():\n",
    "    \"\"\"\n",
    "    Initializing the number and the size of each hidden layers\n",
    "    \"\"\"\n",
    "    def __init__(self,n_hidden_layers,s_hidden_layer):\n",
    "        self.n_hidden_layers = n_hidden_layers # Number of hidden layers\n",
    "        self.s_hidden_layer = s_hidden_layer # Size of the hidden layers\n",
    "    \n",
    "    def sigmoid_activation(self,x):\n",
    "        return 1.0/(1.0 + np.exp(-x)) # 1.0 is for floating for accuracy\n",
    "    \n",
    "    def softmax(self,x):\n",
    "        # returns the output probabilities\n",
    "        exps = np.exp(x)\n",
    "        return exps/np.sum(exps)\n",
    "\n",
    "    def forward_pass(self,x):\n",
    "        \"\"\"\n",
    "        Here we have to initialize weights for each layer \n",
    "        \"\"\"\n",
    "        intialize_weights_and_bias = {}\n",
    "        for i in range(1,self.n_hidden_layers):\n",
    "            intialize_weights_and_bias[\"W\"+str(i)] = np.random.randn(self.s_hidden_layer[i],self.s_hidden_layer[i-1])\n",
    "            intialize_weights_and_bias[\"B\"+str(i)] = np.zeros((self.s_hidden_layer[i],1))\n",
    "\n",
    "        \"\"\" Now we have the weights and biases , we need todo forward passing \n",
    "         for layer 1 to N-1 .\n",
    "\n",
    "         And for the last N^th layer , we need to apply a activation function\n",
    "        \"\"\"\n",
    "        activation_A , activation_H = {} , {}\n",
    "        for i in range(1 , self.n_hidden_layers-1):\n",
    "            activation_A['a'+str(i)] = activation_H['h'+str(i-1)] @ intialize_weights_and_bias['W'+str(i)] + intialize_weights_and_bias[\"B\"+str(i)]  # o = W_1*x + B_1\n",
    "            activation_H['h'+str(i)] = self.sigmoid_activation( activation_A['a'+str(i)]) # y = sigmoid(W_1*x + B_1) \n",
    "\n",
    "        activation_A['a'+str(self.n_hidden_layers-1)] = activation_H['h'+str(self.n_hidden_layers -2)]@intialize_weights_and_bias[\"W\"+str(self.n_hidden_layers-1)] + intialize_weights_and_bias[\"B\"+str(self.n_hidden_layers-1)]\n",
    "        activation_H['h'+str(self.n_hidden_layers-1)] = self.softmax(activation_A['a'+str(self.n_hidden_layers-1)])\n",
    "\n",
    "        return activation_A , activation_H  , intialize_weights_and_bias \n",
    "\n",
    "    def back_propagation(self,y,activation_A ,activation_H):\n",
    "\n",
    "        \"\"\"\n",
    "        First we need to compute the output layer gradient and then backpropagat\n",
    "        through each layer to the initial input layer to find the error \n",
    "        \"\"\"\n",
    "        grad = {}\n",
    "        # Here we are calculating the squared error loss\n",
    "        y_pred = self.softmax(activation_H['h'+str(self.n_hidden_layers - 1)])\n",
    "        y_truth = y.reshape(-1 , 1 ) # reshape to column 1 with any number of rows\n",
    "        \n",
    "        grad['a'+str(self.n_hidden_layers - 1 )] = (y_pred - y_truth) * y_pred * (1 - y_pred)\n",
    "\n",
    "        for i in range(self.n_hidden_layers-1 , 0 , -1):\n",
    "            \n",
    "            # Calculating wrt parameters\n",
    "            grad['W'+str(i)] = np.outer(grad['a'+str(i)],activation_H['h'+str(i-1)])\n",
    "            grad['B'+str(i)] = grad['a'+str(i)]\n",
    "\n",
    "            # Calculating wrt hidden layers\n",
    "\n",
    "            grad['h'+ str(i-1)] = np.dot(self.intialize_weights_and_bias['W'+str(i)],activation_H['h',str(i-1)])\n",
    "            \n",
    "            if i > 1:\n",
    "                tmp = np.exp(-activation_A['a'+str(i-1)])\n",
    "                grad['a'+ str(i-1)] = grad['h'+ str(i-1)] * (tmp/((tmp+1)**2))\n",
    "        \n",
    "        return grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
